{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f16a513c-b4af-4151-b95a-34ebbf0e887f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from gen_disc_networks import Generator, Discriminator\n",
    "from customDataSet import CustomDataset, get_data_loaders\n",
    "from evaluate_rmse import evaluate_rmse\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed130d1-4cc8-47f0-8859-7e706a9553d4",
   "metadata": {},
   "source": [
    "# Creating a cGAN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625d5441-b7eb-441b-bf9b-9f1c92605664",
   "metadata": {},
   "source": [
    "## Definiting the parameters, Generaor and Discriminator networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "263662f0-b6ca-4137-96ca-63618b782141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pre-trained generator loaded\n",
      "The pre-trained discriminator loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_244204/1361460114.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  generator.load_state_dict(torch.load('generator_trained.pth'))\n",
      "/tmp/ipykernel_244204/1361460114.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  discriminator.load_state_dict(torch.load('discriminator_trained.pth'))\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Hyperparameters\n",
    "pixel_size = 256\n",
    "batch_size = 19\n",
    "noise_dim = 1\n",
    "lr = 0.0002 / 10\n",
    "num_epochs = 1\n",
    "ifTrain = True\n",
    "ifSaveModel = True\n",
    "ifCalcRMSE = True\n",
    "ifSaveFig = False\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "generator = Generator(noise_dim).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "if os.path.isfile('generator_trained.pth'):\n",
    "    generator.load_state_dict(torch.load('generator_trained.pth'))\n",
    "    print(\"The pre-trained generator loaded\")\n",
    "if os.path.isfile('discriminator_trained.pth'):\n",
    "    discriminator.load_state_dict(torch.load('discriminator_trained.pth'))\n",
    "    print(\"The pre-trained discriminator loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d199d53b-ec70-46ac-88d9-2af5c27829b0",
   "metadata": {},
   "source": [
    "## Getting the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1acdfc7-6338-43d5-a31b-86946c205364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator summary\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 128, 128]           2,048\n",
      "       BatchNorm2d-2         [-1, 64, 128, 128]             128\n",
      "              ReLU-3         [-1, 64, 128, 128]               0\n",
      "            Conv2d-4          [-1, 128, 64, 64]         131,072\n",
      "       BatchNorm2d-5          [-1, 128, 64, 64]             256\n",
      "              ReLU-6          [-1, 128, 64, 64]               0\n",
      "            Conv2d-7          [-1, 256, 32, 32]         524,288\n",
      "       BatchNorm2d-8          [-1, 256, 32, 32]             512\n",
      "              ReLU-9          [-1, 256, 32, 32]               0\n",
      "           Conv2d-10          [-1, 512, 16, 16]       2,097,152\n",
      "      BatchNorm2d-11          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-12          [-1, 512, 16, 16]               0\n",
      "           Conv2d-13           [-1, 1024, 8, 8]       8,388,608\n",
      "      BatchNorm2d-14           [-1, 1024, 8, 8]           2,048\n",
      "             ReLU-15           [-1, 1024, 8, 8]               0\n",
      "  ConvTranspose2d-16          [-1, 512, 16, 16]       8,388,608\n",
      "      BatchNorm2d-17          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-18          [-1, 512, 16, 16]               0\n",
      "  ConvTranspose2d-19          [-1, 256, 32, 32]       2,097,152\n",
      "      BatchNorm2d-20          [-1, 256, 32, 32]             512\n",
      "             ReLU-21          [-1, 256, 32, 32]               0\n",
      "  ConvTranspose2d-22          [-1, 128, 64, 64]         524,288\n",
      "      BatchNorm2d-23          [-1, 128, 64, 64]             256\n",
      "             ReLU-24          [-1, 128, 64, 64]               0\n",
      "  ConvTranspose2d-25         [-1, 64, 128, 128]         131,072\n",
      "      BatchNorm2d-26         [-1, 64, 128, 128]             128\n",
      "             ReLU-27         [-1, 64, 128, 128]               0\n",
      "  ConvTranspose2d-28          [-1, 1, 256, 256]           1,024\n",
      "             Tanh-29          [-1, 1, 256, 256]               0\n",
      "================================================================\n",
      "Total params: 22,291,200\n",
      "Trainable params: 22,291,200\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 16384.00\n",
      "Forward/backward pass size (MB): 92.50\n",
      "Params size (MB): 85.03\n",
      "Estimated Total Size (MB): 16561.53\n",
      "----------------------------------------------------------------\n",
      "Discriminator summary\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 128, 128]           2,048\n",
      "         LeakyReLU-2         [-1, 64, 128, 128]               0\n",
      "            Conv2d-3          [-1, 128, 64, 64]         131,072\n",
      "       BatchNorm2d-4          [-1, 128, 64, 64]             256\n",
      "         LeakyReLU-5          [-1, 128, 64, 64]               0\n",
      "            Conv2d-6          [-1, 256, 32, 32]         524,288\n",
      "       BatchNorm2d-7          [-1, 256, 32, 32]             512\n",
      "         LeakyReLU-8          [-1, 256, 32, 32]               0\n",
      "            Conv2d-9          [-1, 512, 16, 16]       2,097,152\n",
      "      BatchNorm2d-10          [-1, 512, 16, 16]           1,024\n",
      "        LeakyReLU-11          [-1, 512, 16, 16]               0\n",
      "           Conv2d-12              [-1, 1, 1, 1]         131,072\n",
      "          Sigmoid-13              [-1, 1, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 2,887,424\n",
      "Trainable params: 2,887,424\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.50\n",
      "Forward/backward pass size (MB): 37.00\n",
      "Params size (MB): 11.01\n",
      "Estimated Total Size (MB): 48.51\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#G.get_submodule\n",
    "print(\"Generator summary\")\n",
    "summary(generator, [(1, pixel_size, pixel_size), (noise_dim, pixel_size, pixel_size)])\n",
    "\n",
    "print(\"Discriminator summary\")\n",
    "summary(discriminator, (2, 256, 256))\n",
    "#for parameter in G.parameters():\n",
    "#    print(parameter)\n",
    "# Initialize models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f38cb1-c76f-4139-9b5a-12d3211e3365",
   "metadata": {},
   "source": [
    "## Splitting the dataset to train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db0db091-c210-4dfb-9b42-e28e88940554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    }
   ],
   "source": [
    "# Create dataset instance\n",
    "\n",
    "HOME = os.environ[\"HOME\"]\n",
    "dataset_dpm_path = f\"{HOME}/data/raytracing-dataset/DPM/\"\n",
    "dataset_irt_path = f\"{HOME}/data/raytracing-dataset/IRT2/\"\n",
    "\n",
    "# Create data loader\n",
    "#data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "train_loader, test_loader = get_data_loaders(dataset_dpm_path, dataset_irt_path, batch_size, test_split=0.999)\n",
    "\n",
    "\n",
    "#plt.imshow(image_dpm_np, cmap='gray')\n",
    "#plt.axis('off')\n",
    "#plt.show()\n",
    "#print(len(test_loader.dataset))\n",
    "print(len(train_loader.dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc65a55e-33cc-4eaa-b2d6-b8bab5649e33",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597ff244-1268-4b1a-8744-e4e8734feafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]:   0%|                                                                                                                                              | 0/3 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "if ifTrain == True:\n",
    "    \n",
    "    # Define loss function and optimizers\n",
    "    criterion = nn.BCELoss()\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    g_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch [{epoch+1}/{num_epochs}]', leave=False)\n",
    "        for i, batch in enumerate(progress_bar):\n",
    "            # Unpack the batch\n",
    "            simulated_map, measured_map = batch\n",
    "    \n",
    "            # Move tensors to device\n",
    "            simulated_map = simulated_map.to(device)\n",
    "            measured_map = measured_map.to(device)\n",
    "\n",
    "    \n",
    "            ############################\n",
    "            # Train discriminator\n",
    "            ############################\n",
    "            d_optimizer.zero_grad()\n",
    "    \n",
    "    \n",
    "    \n",
    "            # Train with real data\n",
    "            real_labels = torch.ones(batch_size, 1, device=device)\n",
    "            # Concatenate the simulated map and measured map along the channel dimension\n",
    "            real_inputs = torch.cat((simulated_map, measured_map), dim=1)\n",
    "#            print(f\"real input: {real_inputs.size()}\")\n",
    "            real_outputs = discriminator(real_inputs)\n",
    "    #        print(f\"real label: {real_labels.size()}\")\n",
    "    #        print(f\"real output: {real_outputs.size()}\")\n",
    "            d_loss_real = criterion(real_outputs.squeeze(), real_labels.squeeze())\n",
    "            d_loss_real.backward()\n",
    "    \n",
    "            # Train with fake data\n",
    "            noise_map = torch.randn(batch_size, noise_dim, pixel_size, pixel_size, device=device)\n",
    "            fake_images = generator(simulated_map, noise_map)\n",
    "            fake_labels = torch.zeros(batch_size, 1, device=device)\n",
    "    #        print(f\"fake image size @generator's output: {fake_images.size()}\")\n",
    "    #        print(f\"simulated_map size: {simulated_map.size()}\")\n",
    "            fake_inputs = torch.cat((simulated_map, fake_images), dim=1)\n",
    "            fake_outputs = discriminator(fake_inputs)\n",
    "    #        print(f\"fake label: {fake_labels.size()}\")\n",
    "    #        print(f\"fake output: {fake_outputs.size()}\")\n",
    "            d_loss_fake = criterion(fake_outputs.squeeze(), fake_labels.squeeze())\n",
    "            d_loss_fake.backward()\n",
    "    \n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            d_optimizer.step()\n",
    "    \n",
    "            ############################\n",
    "            # Train generator\n",
    "            ############################\n",
    "            g_optimizer.zero_grad()\n",
    "    \n",
    "            # Generate fake images\n",
    "            noise_map = torch.randn(batch_size, noise_dim, pixel_size, pixel_size, device=device)\n",
    "            fake_images = generator(simulated_map, noise_map)\n",
    "    \n",
    "            # Train generator with discriminator feedback\n",
    "            fake_inputs = torch.cat((simulated_map, fake_images), dim=1)\n",
    "            outputs = discriminator(fake_inputs)\n",
    "            g_loss = criterion(outputs.squeeze(), real_labels.squeeze())\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "    \n",
    "    \n",
    "    \n",
    "            ############################\n",
    "            # Print losses\n",
    "            ############################\n",
    "#            if i % 10 == 0:\n",
    "#                print(f\"Epoch [{epoch}/{num_epochs}], Step [{i}/{len(train_loader)}], \"\n",
    "#                      f\"D_loss: {d_loss.item():.4f}, G_loss: {g_loss.item():.4f}\")\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}], Step [{i}/{len(train_loader)}], \"\n",
    "                      f\"D_loss: {d_loss.item():.4f}, G_loss: {g_loss.item():.4f}\")\n",
    "        if ifSaveModel == True:\n",
    "            torch.save(discriminator.state_dict(), 'discriminator_trained.pth')\n",
    "            torch.save(discriminator, 'discriminator_entire_model.pth')\n",
    "            torch.save(generator.state_dict(), 'generator_trained.pth')\n",
    "            torch.save(generator, 'generator_entire_model.pth')\n",
    "            print(f\"Model saved at epoch {epoch}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdd89ea-a7ee-4222-a06e-27d8d6932a6d",
   "metadata": {},
   "source": [
    "# Evaluating the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f421f062-4265-4dec-b57a-472b298a9eb0",
   "metadata": {},
   "source": [
    "## Showing a sample synthetic map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be576201-0e0d-4879-939a-a9e149d13060",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Choose a random index\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Choose a random index#\u001b[39;00m\n\u001b[1;32m     17\u001b[0m random_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10372\u001b[39m\n\u001b[0;32m---> 18\u001b[0m random_index \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(test_loader\u001b[38;5;241m.\u001b[39mdataset) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom index: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Get the image at the random index\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate a new image\n",
    "#from synthetic_map import generate_image\n",
    "def generate_image(generator, simulated_map, noise_dim):\n",
    "    # Prepare simulated map and noise map\n",
    "    simulated_map = simulated_map.to(device)\n",
    "    noise_map = torch.randn(1, noise_dim, pixel_size, pixel_size, device=device)\n",
    "\n",
    "    # Generate image\n",
    "    with torch.no_grad():\n",
    "        generated_image = generator(simulated_map, noise_map)\n",
    "   # Denormalize the generated image from [-1, 1] to [0, 1]\n",
    "#    generated_image = (generated_image + 1) / 2 \n",
    "    return generated_image\n",
    "\n",
    "# Choose a random index\n",
    "# Choose a random index#\n",
    "random_index = 10372\n",
    "random_index = random.randint(0, len(test_loader.dataset) - 1)\n",
    "print(f\"Random index: {random_index}\")\n",
    "\n",
    "# Get the image at the random index\n",
    "image_dpm, image_irt = test_loader.dataset[random_index]\n",
    "\n",
    "# Convert tensor to numpy array and remove batch dimension\n",
    "image_dpm_np = image_dpm.squeeze().numpy()\n",
    "image_irt_np = image_irt.squeeze().numpy()\n",
    "\n",
    "simulated_map = torch.from_numpy(image_dpm_np).reshape(1, 1 , pixel_size, pixel_size)\n",
    "synthetic_image = generate_image(generator, simulated_map, noise_dim).to(\"cpu\")\n",
    "synthetic_image = synthetic_image.squeeze().numpy()\n",
    "\n",
    "# Plot the image\n",
    "# Create a figure with 1 row and 3 columns\n",
    "# Plot each image in a subplot\n",
    "fig = plt.figure(figsize=(10, 7)) \n",
    "fig.add_subplot(1, 3, 1) \n",
    "plt.imshow(image_irt_np , cmap='gray')\n",
    "plt.axis('off')  # Hide axes for a cleaner look\n",
    "plt.title('Ray-Tracing')\n",
    "\n",
    "fig.add_subplot(1, 3, 2) \n",
    "plt.imshow(image_dpm_np , cmap='gray')\n",
    "plt.axis('off')  # Hide axes for a cleaner look\n",
    "plt.title('Dominant path loss')\n",
    "\n",
    "fig.add_subplot(1, 3, 3)\n",
    "plt.imshow(synthetic_image, cmap='gray')\n",
    "plt.axis('off')  # Hide axes for a cleaner look\n",
    "plt.title('Synthetic radio map')\n",
    "\n",
    "# Adjust layout to avoid overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "# Save the full figure...\n",
    "if ifSaveFig:\n",
    "    fig.savefig(\"/home/ehsan/git/juelich-presentation/pics/all_methods_comparison.png\")\n",
    "\n",
    "\n",
    "#img = Image.fromarray(np.uint8(255 * image_irt_np))  # no opencv required\n",
    "#if ifSaveFig:\n",
    "#    img.save(\"/home/ehsan/git/juelich-presentation/pics/irt.png\")\n",
    "    \n",
    "#img = Image.fromarray(np.uint8(255 * image_dpm_np))  # no opencv required\n",
    "#if ifSaveFig:\n",
    "#    img.save(\"/home/ehsan/git/juelich-presentation/pics/dpm.png\")\n",
    "    \n",
    "#img = Image.fromarray(np.uint8(255 * synthetic_image))  # no opencv required\n",
    "#if ifSaveFig:\n",
    "#    img.save(\"/home/ehsan/git/juelich-presentation/pics/synthetic.png\")\n",
    "\n",
    "\n",
    "### Plot the error images\n",
    "fig = plt.figure(figsize=(10, 7)) \n",
    "fig.add_subplot(1, 2, 1) \n",
    "error_dpm = 100*(image_irt_np -image_dpm_np) / image_irt_np\n",
    "plt.imshow(error_dpm , cmap='gray')\n",
    "plt.axis('off')  # Hide axes for a cleaner look\n",
    "plt.title('Dominant path loss')\n",
    "\n",
    "fig.add_subplot(1, 2, 2) \n",
    "error_synthetic = 100*(image_irt_np - synthetic_image) / image_irt_np\n",
    "plt.imshow(error_synthetic , cmap='gray')\n",
    "plt.axis('off')  # Hide axes for a cleaner look\n",
    "plt.title('Synthetic radio map')\n",
    "\n",
    "# Adjust layout to avoid overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "# Save the full figure...\n",
    "if ifSaveFig:\n",
    "    fig.savefig(\"/home/ehsan/git/juelich-presentation/pics/error_image_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88a62f7-a825-4bec-b0af-5f39b88234f6",
   "metadata": {},
   "source": [
    "random_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59acdb2e-8db0-454b-8ed4-fb9d236aef1d",
   "metadata": {},
   "source": [
    "## Calculate the RMSE on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3fe1091d-7dc8-476d-8109-b89830a4ceba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|‚ñè                                                   | 10/2949 [00:11<56:35,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average RMSE for generated images: 0.31292821698639384\n",
      "Average RMSE for simulated maps: 0.09632769258220515\n",
      "Improvement in RMSE: -224.8580014717407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if ifCalcRMSE:\n",
    "    avg_rmse_generated, avg_rmse_simulated = evaluate_rmse(generator, test_loader, noise_dim)\n",
    "    print(f\"Average RMSE for generated images: {avg_rmse_generated}\")\n",
    "    print(f\"Average RMSE for simulated maps: {avg_rmse_simulated}\")\n",
    "    improvement = (1 - avg_rmse_generated / avg_rmse_simulated)*100\n",
    "    print(f\"Improvement in RMSE: {improvement}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52f19d3-188d-44b6-9df1-049da715c48a",
   "metadata": {},
   "source": [
    "## Visualizing the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "39f4a6bb-670c-40a9-9801-cdb97c60c9f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchviz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_dot\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Create a dummy input for the generator (batch size 1)\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchviz'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "import torch.onnx\n",
    "\n",
    "\n",
    "# Create a dummy input for the generator (batch size 1)\n",
    "dummy_input = torch.randn(1, 1, 256, 256).to(device)\n",
    "# Generate the visualization\n",
    "noise_map = torch.randn(1, noise_dim, pixel_size, pixel_size, device=device).to(device)\n",
    "output = generator(dummy_input, noise_map)\n",
    "model_viz = make_dot(output, params=dict(generator.named_parameters()))\n",
    "# Render the graph to a file\n",
    "model_viz.render(\"generator_architecture\", format=\"png\")\n",
    "\n",
    "# Display the graph inline (optional, for Jupyter/Colab)\n",
    "from IPython.display import Image\n",
    "Image(filename=\"generator_architecture.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5e0c17-5180-4256-b5b5-b8b01c7507f6",
   "metadata": {},
   "source": [
    "## Visualizing the discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ffa036-8345-4a67-ad03-5f18e969ccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy input for the discriminator (batch size 1, single-channel grayscale image)\n",
    "dummy_input = torch.randn(1, 2, 256, 256).to(device)\n",
    "\n",
    "# Generate the visualization\n",
    "#real_inputs = torch.cat((simulated_map, measured_map), dim=1)\n",
    "output = discriminator(dummy_input)\n",
    "model_viz = make_dot(output, params=dict(discriminator.named_parameters()))\n",
    "\n",
    "# Render the graph to a file\n",
    "model_viz.render(\"discriminator_architecture\", format=\"png\")\n",
    "\n",
    "# Display the graph inline (optional, for Jupyter/Colab)\n",
    "Image(filename=\"discriminator_architecture.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
